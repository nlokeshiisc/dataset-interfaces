{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17a1f52-5eea-44f1-a57d-2791dbd7b933",
   "metadata": {},
   "source": [
    "# Dataset Interfaces Example Notebook\n",
    "\n",
    "In this notebook, we provide an example of creating a dataset interface for a subset of three classes of the ImageNet dataset. We also demonstrate how to create/load in a pre-trained text encoder containing our learred tokens for ImageNet* to generate your own counterfactual examples for ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13a2e8-14f6-4168-af5a-b9d091418d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccada6-b8dd-4471-a2d2-66faa8844ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# comment this out if you are using the pip package\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset_interfaces import utils\n",
    "from dataset_interfaces import run_textual_inversion\n",
    "from dataset_interfaces import generate\n",
    "import dataset_interfaces.imagenet_utils as in_utils\n",
    "import dataset_interfaces.inference_utils as infer_utils\n",
    "from pathlib import Path\n",
    "\n",
    "# set root to ImageNet dataset\n",
    "print(\"Currently hardcoded to base folder\")\n",
    "IMAGENET_ROOT = \"/raid/infolab/nlokesh/dataset-interfaces/data/imagenet_star/base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56053d2-00be-4855-ac02-3e56fc73e1d3",
   "metadata": {},
   "source": [
    "To easily use the learned tokens in text prompts, we load the learned token-embedding pairs into a tokenizer and corresponding text encoder. Below we define the path where we will store the tokenizer and encoder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4aa3d-b8df-45af-9ed7-7925348cf5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where to store an encoder, which we will load in with the learned tokens\n",
    "encoder_root = \"./encoder_root\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff10c3e1-ce23-4e14-9748-33f60fd22c37",
   "metadata": {},
   "source": [
    "### Option 1: Construct Dataset Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09c972-45d8-447b-9300-83a6ca0ed237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a subset of ImageNet classes\n",
    "classes = [236, 651, 10]\n",
    "class_names = [in_utils.IMAGENET_COMMON_CLASS_NAMES[c] for c in classes]\n",
    "tokens = [f\"<{class_names[i]}-{i}>\" for i in range(len(class_names))]\n",
    "print(tokens)\n",
    "\n",
    "# train_data_dirs = [os.path.join(IMAGENET_ROOT, \"train\", in_utils.IMAGENET_IDX_TO_SYNSET[str(c)]['id']) for c in classes]\n",
    "train_data_dirs = [os.path.join(IMAGENET_ROOT, in_utils.IMAGENET_IDX_TO_SYNSET[str(c)]['id']) for c in classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d00ce-5ad4-427a-ae05-a865e5f02fb7",
   "metadata": {},
   "source": [
    "#### Run textual inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc88e26-5d67-4cd9-ac26-85c8126e08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = []\n",
    "for i in range(len(classes)):\n",
    "    \n",
    "    # runs textual inversion on a single class\n",
    "    embed = run_textual_inversion(train_data_dirs[i],\n",
    "        token = tokens[i],\n",
    "        class_name = class_names[i]\n",
    "    )\n",
    "    \n",
    "    embeds.append(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c49d3-7a6d-477b-8f65-f7bb53d32d97",
   "metadata": {},
   "source": [
    "#### Add to tokenizer and text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cabf0-b3a3-4475-b500-db13fa59d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_utils.create_encoder(embeds=embeds, tokens=tokens, class_names=class_names, encoder_root=encoder_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0324f-0f50-4930-86ce-caedf07d96fe",
   "metadata": {},
   "source": [
    "### Option 2: Create Encoder Root for ImageNet\n",
    "To use our learned tokens for the ImageNet (ImageNet*), we save a tokenizer and text encoder with the 1k tokens. This could take 6+ minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7eb9f-9f55-4d1b-b7a1-920689a03e20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the tokens from huggingface\n",
    "!wget https://huggingface.co/datasets/madrylab/imagenet-star-tokens/resolve/main/tokens.zip\n",
    "!unzip tokens.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139fc30-80be-4448-b248-38be044eb07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = \"./tokens\"\n",
    "infer_utils.create_imagenet_star_encoder(token_path, encoder_root=\"./encoder_root_imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890bbc43-5d5b-4b00-9ff1-e43239936af9",
   "metadata": {},
   "source": [
    "### Generate Counterfactual Examples\n",
    "\n",
    "To use our learned class tokens for the ImageNet Dataset (ImageNet*), keep `use_provided=True` <br> \n",
    "To use the tokens learned in the cells above, set `use_provided=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17797e-8c55-4f70-8d0c-34f14c6fda85",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_provided = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58669e7-a470-442f-8b32-476c6d6dd414",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_provided:\n",
    "    classes = [236, 651, 10]\n",
    "    class_names = [in_utils.IMAGENET_COMMON_CLASS_NAMES[c] for c in classes]\n",
    "    root = \"./encoder_root_imagenet\"\n",
    "\n",
    "    \n",
    "else:\n",
    "    classes = [0, 1, 2]\n",
    "    class_names = class_names\n",
    "    root = encoder_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad903ccd-e60c-4065-ab25-5d52f6ccfbb4",
   "metadata": {},
   "source": [
    "#### A small set of distribution shifts, as examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171eb77-28ff-4d34-9f6b-e3fd6d8b3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = [\"base\", \"in the grass\", \"in the snow\", \"in bright sunlight\", \"oil painting\"]\n",
    "prompts = [\n",
    "    \"a photo of a <TOKEN>\",\n",
    "    \"a photo of a <TOKEN> in the grass\",\n",
    "    \"a photo of a <TOKEN> in the snow\",\n",
    "    \"a photo of a <TOKEN> in bright sunlight\",\n",
    "    \"an oil painting of a <TOKEN>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce16ba7-1599-44d0-9898-8b386e7aab7c",
   "metadata": {},
   "source": [
    "#### Generating counterfactual examples in the shifts above for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be10d38-ca8a-465f-907f-311278a4d318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs = []\n",
    "seed = 0\n",
    "for c in classes:\n",
    "    imgs_class = generate(root, c, prompts, num_samples=1, random_seed=range(seed, seed+len(prompts)))\n",
    "    imgs_class = generate(root, c, prompts, num_samples=1, random_seed=range(seed, seed+len(prompts)))\n",
    "    imgs_class = [imgs[0] for imgs in imgs_class]\n",
    "    seed += len(prompts)\n",
    "        \n",
    "    imgs.append(imgs_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3e73c-983f-42d0-a98a-cc61da3f283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_samples(imgs, class_names, shifts, dpi=200, figsize=(6,4), fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b2ae1-fcaa-4da5-8db9-b9e97385934d",
   "metadata": {},
   "source": [
    "### CLIP Metrics\n",
    "To directly evaluate the quality of the generated image, we use CLIP similarity to quantify the presence of the object of interest and desired distribution shift in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a544fd-5e0b-4b22-b84f-abcabd9a22fc",
   "metadata": {},
   "source": [
    "We can measure the extent to which generated images for the class \"doberman\" contain a doberman as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45bf85-5784-4918-8f3f-772e8a6abd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_utils.clip_similarity(imgs[0], \"a photo of a doberman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290aabbe-48a1-4ccb-88db-63b91d133058",
   "metadata": {},
   "source": [
    "We can measure the extent to which generated images in the grass are indeed in the grass as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab009c4e-6793-41d9-ba62-95a2e9355ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_utils.clip_similarity([imgs[i][1] for i in range(3)], \"a photo in the grass\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
