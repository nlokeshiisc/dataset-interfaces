
{476: {'loss': tensor(0.1056, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4224, grad_fn=<ToCopyBackward0>)}, 84: {'loss': tensor(0.1597, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.6387, grad_fn=<ToCopyBackward0>)}, 340: {'loss': tensor(0.1247, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4988, grad_fn=<ToCopyBackward0>)}, 145: {'loss': tensor(0.0948, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3794, grad_fn=<ToCopyBackward0>)}, 105: {'loss': tensor(0.1373, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.5493, grad_fn=<ToCopyBackward0>)}, 94: {'loss': tensor(0.0578, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.2312, grad_fn=<ToCopyBackward0>)}, 1: {'loss': tensor(0.0574, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.2295, grad_fn=<ToCopyBackward0>)}, 327: {'loss': tensor(0.0977, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3909, grad_fn=<ToCopyBackward0>)}, 561: {'loss': tensor(0.1087, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4348, grad_fn=<ToCopyBackward0>)}, 323: {'loss': tensor(0.2050, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.8198, grad_fn=<ToCopyBackward0>)}, 936: {'loss': tensor(0.1086, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4346, grad_fn=<ToCopyBackward0>)}, 251: {'loss': tensor(0.0658, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.2632, grad_fn=<ToCopyBackward0>)}, 130: {'loss': tensor(0.0910, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3640, grad_fn=<ToCopyBackward0>)}, 296: {'loss': tensor(0.1159, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4636, grad_fn=<ToCopyBackward0>)}, 141: {'loss': tensor(0.0729, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.2915, grad_fn=<ToCopyBackward0>)}, 388: {'loss': tensor(0.1133, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4534, grad_fn=<ToCopyBackward0>)}, 387: {'loss': tensor(0.1146, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4583, grad_fn=<ToCopyBackward0>)}, 802: {'loss': tensor(0.1122, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4490, grad_fn=<ToCopyBackward0>)}, 661: {'loss': tensor(0.1076, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4304, grad_fn=<ToCopyBackward0>)}, 97: {'loss': tensor(0.0753, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3010, grad_fn=<ToCopyBackward0>)}}
{476: {'loss': tensor(0.1151, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4604, grad_fn=<ToCopyBackward0>)}, 84: {'loss': tensor(0.1504, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.6016, grad_fn=<ToCopyBackward0>)}, 340: {'loss': tensor(0.1799, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.7197, grad_fn=<ToCopyBackward0>)}, 145: {'loss': tensor(0.0900, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3601, grad_fn=<ToCopyBackward0>)}, 105: {'loss': tensor(0.0817, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3269, grad_fn=<ToCopyBackward0>)}, 94: {'loss': tensor(0.0344, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.1376, grad_fn=<ToCopyBackward0>)}, 1: {'loss': tensor(0.0601, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.2405, grad_fn=<ToCopyBackward0>)}, 327: {'loss': tensor(0.1046, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4182, grad_fn=<ToCopyBackward0>)}, 561: {'loss': tensor(0.0770, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3079, grad_fn=<ToCopyBackward0>)}, 323: {'loss': tensor(0.1191, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4766, grad_fn=<ToCopyBackward0>)}, 936: {'loss': tensor(0.1278, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.5112, grad_fn=<ToCopyBackward0>)}, 251: {'loss': tensor(0.1125, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4502, grad_fn=<ToCopyBackward0>)}, 130: {'loss': tensor(0.0785, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3140, grad_fn=<ToCopyBackward0>)}, 296: {'loss': tensor(0.0276, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.1105, grad_fn=<ToCopyBackward0>)}, 141: {'loss': tensor(0.0895, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3579, grad_fn=<ToCopyBackward0>)}, 388: {'loss': tensor(0.1395, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.5581, grad_fn=<ToCopyBackward0>)}, 387: {'loss': tensor(0.1288, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.5151, grad_fn=<ToCopyBackward0>)}, 802: {'loss': tensor(0.0953, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.3813, grad_fn=<ToCopyBackward0>)}, 661: {'loss': tensor(0.1081, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4324, grad_fn=<ToCopyBackward0>)}, 97: {'loss': tensor(0.1173, grad_fn=<ToCopyBackward0>), 'loss_1': tensor(0.4692, grad_fn=<ToCopyBackward0>)}}
Traceback (most recent call last):
  File "/raid/infolab/nlokesh/dataset-interfaces/notebooks/Extract_Better_Embeds.py", line 135, in <module>
    embeds = run_textual_inversion_plus(
  File "/raid/infolab/nlokesh/dataset-interfaces/notebooks/../dataset_interfaces/textual_inversion.py", line 1115, in run_textual_inversion_plus
    accelerator.backward(loss)
  File "/raid/infolab/nlokesh/anaconda3/envs/dsi/lib/python3.10/site-packages/accelerate/accelerator.py", line 1851, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/raid/infolab/nlokesh/anaconda3/envs/dsi/lib/python3.10/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/raid/infolab/nlokesh/anaconda3/envs/dsi/lib/python3.10/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt