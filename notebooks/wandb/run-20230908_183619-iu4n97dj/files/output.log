unet/diffusion_pytorch_model.safetensors not found
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.83it/s]
/raid/infolab/nlokesh/anaconda3/envs/dsi/lib/python3.10/site-packages/diffusers/configuration_utils.py:239: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.
  deprecate("config-passed-as-path", "1.0.0", deprecation_message, standard_warn=False)
Steps:   0%|          | 0/5 [00:00<?, ?it/s]
[('a photo of a cool <carousel-0> in the rain',), ('a photo of a cool <carousel-0> in the rain',), ('a photo of a cool <carousel-0> in the snow',), ('a photo of a cool <carousel-0> in the snow',), ('a photo of a cool <carousel-0> at night',), ('a photo of a cool <carousel-0> in the forest',), ('a photo of a cool <carousel-0> in the rain',), ('a photo of a cool <carousel-0> at night',), ('a photo of a cool <carousel-0> at night',), ('a photo of a cool <carousel-0> at night',), ('a photo of a cool <carousel-0> in the rain',), ('a photo of a cool <carousel-0> at night',), ('a photo of a cool <carousel-0> in the rain',), ('a photo of a cool <carousel-0> in the fog',), ('a photo of a cool <carousel-0> in the rain',), ('a photo of a cool <carousel-0> in the forest',), ('a photo of a cool <carousel-0> in the forest',), ('a photo of a cool <carousel-0> in the snow',), ('a photo of a cool <carousel-0> in the forest',), ('a photo of a cool <carousel-0> in the rain',)]
  lambda data: self._console_raw_callback("stderr", data),0<00:00,  6.88it/s]
/raid/infolab/nlokesh/anaconda3/envs/dsi/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2129: UserWarning: Run (dd98z92y) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),
Steps:   0%|          | 0/5 [00:00<?, ?it/s]
base_prompt = a photo of one {}
Steps:   0%|          | 0/5 [00:00<?, ?it/s]unet/diffusion_pytorch_model.safetensors not found
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.28it/s]

[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.06it/s]
texts = ['a cropped photo of a <carousel-0> in the rain']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the fog']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the fog', 'a cropped photo of a <carousel-0> in the rain']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the fog', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the forest']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the fog', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the forest']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the fog', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the snow']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the fog', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the forest']
texts = ['a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> at night', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the fog', 'a cropped photo of a <carousel-0> in the rain', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the snow', 'a cropped photo of a <carousel-0> in the forest', 'a cropped photo of a <carousel-0> in the rain']
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.29it/s]

Steps:   0%|          | 0/5 [05:33<?, ?it/s]â–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.01it/s]
Steps:   0%|          | 0/5 [00:00<?, ?it/s]
[('a cropped photo of a <carousel-0> in the rain',), ('a cropped photo of a <carousel-0> in the rain',), ('a cropped photo of a <carousel-0> in the snow',), ('a cropped photo of a <carousel-0> in the snow',), ('a cropped photo of a <carousel-0> at night',), ('a cropped photo of a <carousel-0> in the forest',), ('a cropped photo of a <carousel-0> in the rain',), ('a cropped photo of a <carousel-0> at night',), ('a cropped photo of a <carousel-0> at night',), ('a cropped photo of a <carousel-0> at night',), ('a cropped photo of a <carousel-0> in the rain',), ('a cropped photo of a <carousel-0> at night',), ('a cropped photo of a <carousel-0> in the rain',), ('a cropped photo of a <carousel-0> in the fog',), ('a cropped photo of a <carousel-0> in the rain',), ('a cropped photo of a <carousel-0> in the forest',), ('a cropped photo of a <carousel-0> in the forest',), ('a cropped photo of a <carousel-0> in the snow',), ('a cropped photo of a <carousel-0> in the forest',), ('a cropped photo of a <carousel-0> in the rain',)]
batch = {'input_ids_list': [tensor([[49406,   320,  4852,  1125,   539,   518, 49428,   530,   518,  9417,
         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320,  4852,  1125,   539,   518, 49428,   536,   930, 49407,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320,  4852,  1125,   539,   518, 49428,   530,   518,  2443,
         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320,  4852,  1125,   539,   518, 49428,   530,   518,  4167,
         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320,  4852,  1125,   539,   518, 49428,   530,   518,  2583,
         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]])], 'text_prompts': [('a bright photo of the <carousel-0> in the fog',), ('a bright photo of the <carousel-0> at night',), ('a bright photo of the <carousel-0> in the rain',), ('a bright photo of the <carousel-0> in the forest',), ('a bright photo of the <carousel-0> in the snow',)], 'true_beta_index': tensor([2]), 'object_text': ['<carousel-0>'], 'background_text': ['in the rain'], 'pixel_values': tensor([[[[-0.0588, -0.0667, -0.0902,  ..., -0.5371, -0.5527, -0.5605],
          [-0.0980, -0.1059, -0.1294,  ..., -0.5449, -0.5527, -0.5605],
          [-0.1451, -0.1530, -0.1765,  ..., -0.5605, -0.5688, -0.5767],
          ...,
          [-0.4587, -0.4587, -0.4666,  ..., -0.0902, -0.1059, -0.1137],
          [-0.4746, -0.4824, -0.4902,  ..., -0.1137, -0.1216, -0.1216],
          [-0.4824, -0.4902, -0.4980,  ..., -0.0980, -0.0980, -0.0980]],
         [[-0.8823, -0.8667, -0.8511,  ..., -0.5215, -0.5371, -0.5449],
          [-0.9292, -0.9136, -0.8979,  ..., -0.5293, -0.5371, -0.5449],
          [-0.9844, -0.9766, -0.9531,  ..., -0.5449, -0.5527, -0.5605],
          ...,
          [-0.9214, -0.9214, -0.9214,  ...,  0.2393,  0.2235,  0.2157],
          [-0.9292, -0.9292, -0.9214,  ...,  0.2157,  0.2079,  0.2079],
          [-0.9292, -0.9292, -0.9214,  ...,  0.2313,  0.2313,  0.2313]],
         [[-0.7646, -0.7568, -0.7490,  ..., -0.5449, -0.5605, -0.5688],
          [-0.8115, -0.8037, -0.7881,  ..., -0.5449, -0.5527, -0.5605],
          [-0.8589, -0.8511, -0.8354,  ..., -0.5449, -0.5527, -0.5688],
          ...,
          [-0.8037, -0.8037, -0.8037,  ...,  0.5293,  0.5215,  0.5137],
          [-0.7959, -0.7959, -0.7959,  ...,  0.5137,  0.5059,  0.5059],
          [-0.7959, -0.7959, -0.7959,  ...,  0.5293,  0.5293,  0.5293]]]],
       device='cuda:0', dtype=torch.float16)}
Steps:   0%|          | 0/5 [00:00<?, ?it/s]unet/diffusion_pytorch_model.safetensors not found
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.71it/s]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.44it/s]
batch[input_ids_list] = [tensor([[49406,   320,  2660,   268,   705,  1125,   539,   518, 49428,   530,
           518,  9417, 49407,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320,  2660,   268,   705,  1125,   539,   518, 49428,   536,
           930, 49407,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320,  2660,   268,   705,  1125,   539,   518, 49428,   530,
           518,  2443, 49407,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320,  2660,   268,   705,  1125,   539,   518, 49428,   530,
           518,  4167, 49407,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320,  2660,   268,   705,  1125,   539,   518, 49428,   530,
           518,  2583, 49407,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]])]
batch[text_prompts] = [('a close-up photo of the <carousel-0> in the fog',), ('a close-up photo of the <carousel-0> at night',), ('a close-up photo of the <carousel-0> in the rain',), ('a close-up photo of the <carousel-0> in the forest',), ('a close-up photo of the <carousel-0> in the snow',)]
batch[true_beta_index] = tensor([2])
batch[object_text] = ['<carousel-0>']
batch[background_text] = ['in the rain']
batch[pixel_values] = tensor([[[[-0.5605, -0.5527, -0.5371,  ..., -0.0902, -0.0667, -0.0588],
          [-0.5605, -0.5527, -0.5449,  ..., -0.1294, -0.1059, -0.0980],
          [-0.5767, -0.5688, -0.5605,  ..., -0.1765, -0.1530, -0.1451],
          ...,
          [-0.1137, -0.1059, -0.0902,  ..., -0.4666, -0.4587, -0.4587],
          [-0.1216, -0.1216, -0.1137,  ..., -0.4902, -0.4824, -0.4746],
          [-0.0980, -0.0980, -0.0980,  ..., -0.4980, -0.4902, -0.4824]],
         [[-0.5449, -0.5371, -0.5215,  ..., -0.8511, -0.8667, -0.8823],
          [-0.5449, -0.5371, -0.5293,  ..., -0.8979, -0.9136, -0.9292],
          [-0.5605, -0.5527, -0.5449,  ..., -0.9531, -0.9766, -0.9844],
          ...,
          [ 0.2157,  0.2235,  0.2393,  ..., -0.9214, -0.9214, -0.9214],
          [ 0.2079,  0.2079,  0.2157,  ..., -0.9214, -0.9292, -0.9292],
          [ 0.2313,  0.2313,  0.2313,  ..., -0.9214, -0.9292, -0.9292]],
         [[-0.5688, -0.5605, -0.5449,  ..., -0.7490, -0.7568, -0.7646],
          [-0.5605, -0.5527, -0.5449,  ..., -0.7881, -0.8037, -0.8115],
          [-0.5688, -0.5527, -0.5449,  ..., -0.8354, -0.8511, -0.8589],
          ...,
          [ 0.5137,  0.5215,  0.5293,  ..., -0.8037, -0.8037, -0.8037],
          [ 0.5059,  0.5059,  0.5137,  ..., -0.7959, -0.7959, -0.7959],
          [ 0.5293,  0.5293,  0.5293,  ..., -0.7959, -0.7959, -0.7959]]]],
       device='cuda:0', dtype=torch.float16)
[('a close-up photo of the <carousel-0> in the fog',), ('a close-up photo of the <carousel-0> at night',), ('a close-up photo of the <carousel-0> in the rain',), ('a close-up photo of the <carousel-0> in the forest',), ('a close-up photo of the <carousel-0> in the snow',)]
Steps:   0%|          | 0/5 [00:00<?, ?it/s]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.19it/s]
keys = dict_keys(['input_ids_list', 'text_prompts', 'true_beta_index', 'object_text', 'background_text', 'pixel_values'])
batch[input_ids_list] = [tensor([[49406,   320, 31139,  1125,   539,   320, 49428,   530,   518,  9417,
         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320, 31139,  1125,   539,   320, 49428,   536,   930, 49407,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320, 31139,  1125,   539,   320, 49428,   530,   518,  2443,
         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320, 31139,  1125,   539,   320, 49428,   530,   518,  4167,
         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]]), tensor([[49406,   320, 31139,  1125,   539,   320, 49428,   530,   518,  2583,
         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0]])]
batch[text_prompts] = [('a cropped photo of a <carousel-0> in the fog',), ('a cropped photo of a <carousel-0> at night',), ('a cropped photo of a <carousel-0> in the rain',), ('a cropped photo of a <carousel-0> in the forest',), ('a cropped photo of a <carousel-0> in the snow',)]
batch[true_beta_index] = tensor([2])
batch[object_text] = ['<carousel-0>']
batch[background_text] = ['in the rain']
batch[pixel_values] = tensor([[[[-0.5605, -0.5527, -0.5371,  ..., -0.0902, -0.0667, -0.0588],
          [-0.5605, -0.5527, -0.5449,  ..., -0.1294, -0.1059, -0.0980],
          [-0.5767, -0.5688, -0.5605,  ..., -0.1765, -0.1530, -0.1451],
          ...,
          [-0.1137, -0.1059, -0.0902,  ..., -0.4666, -0.4587, -0.4587],
          [-0.1216, -0.1216, -0.1137,  ..., -0.4902, -0.4824, -0.4746],
          [-0.0980, -0.0980, -0.0980,  ..., -0.4980, -0.4902, -0.4824]],
         [[-0.5449, -0.5371, -0.5215,  ..., -0.8511, -0.8667, -0.8823],
          [-0.5449, -0.5371, -0.5293,  ..., -0.8979, -0.9136, -0.9292],
          [-0.5605, -0.5527, -0.5449,  ..., -0.9531, -0.9766, -0.9844],
          ...,
          [ 0.2157,  0.2235,  0.2393,  ..., -0.9214, -0.9214, -0.9214],
          [ 0.2079,  0.2079,  0.2157,  ..., -0.9214, -0.9292, -0.9292],
          [ 0.2313,  0.2313,  0.2313,  ..., -0.9214, -0.9292, -0.9292]],
         [[-0.5688, -0.5605, -0.5449,  ..., -0.7490, -0.7568, -0.7646],
          [-0.5605, -0.5527, -0.5449,  ..., -0.7881, -0.8037, -0.8115],
          [-0.5688, -0.5527, -0.5449,  ..., -0.8354, -0.8511, -0.8589],
          ...,
          [ 0.5137,  0.5215,  0.5293,  ..., -0.8037, -0.8037, -0.8037],
          [ 0.5059,  0.5059,  0.5137,  ..., -0.7959, -0.7959, -0.7959],
          [ 0.5293,  0.5293,  0.5293,  ..., -0.7959, -0.7959, -0.7959]]]],
       device='cuda:0', dtype=torch.float16)
[('a cropped photo of a <carousel-0> in the fog',), ('a cropped photo of a <carousel-0> at night',), ('a cropped photo of a <carousel-0> in the rain',), ('a cropped photo of a <carousel-0> in the forest',), ('a cropped photo of a <carousel-0> in the snow',)]

Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.88it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
batch[text_prompts] = [('a photo of my <carousel-0> in the fog',), ('a photo of my <carousel-0> at night',), ('a photo of my <carousel-0> in the rain',), ('a photo of my <carousel-0> in the forest',), ('a photo of my <carousel-0> in the snow',)]
batch[true_beta_index] = tensor([2])
batch[object_text] = ['<carousel-0>']
batch[background_text] = ['in the rain']
[('a photo of my <carousel-0> in the fog',), ('a photo of my <carousel-0> at night',), ('a photo of my <carousel-0> in the rain',), ('a photo of my <carousel-0> in the forest',), ('a photo of my <carousel-0> in the snow',)]
Steps:   0%|          | 0/5 [00:00<?, ?it/s]unet/diffusion_pytorch_model.safetensors not found
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.22it/s]

[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.98it/s]
Steps:   0%|          | 0/5 [03:06<?, ?it/s]
Steps:   0%|          | 0/5 [03:54<?, ?it/s]
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]
  0%|          | 0/50 [00:00<?, ?it/s]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.10it/s]
Steps:   0%|          | 0/5 [00:00<?, ?it/s]
loaded clip model
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.60it/s]
  0%|          | 0/50 [00:00<?, ?it/s]

Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.56it/s]
  0%|          | 0/50 [00:00<?, ?it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
Steps:   0%|          | 0/5 [00:00<?, ?it/s]unet/diffusion_pytorch_model.safetensors not found
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.11it/s]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.82it/s]
  0%|          | 0/50 [00:00<?, ?it/s]â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.82it/s]

[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
Steps:   0%|          | 0/5 [00:00<?, ?it/s]
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]
pipe = StableDiffusionPipeline {
  "_class_name": "StableDiffusionPipeline",
  "_diffusers_version": "0.20.0",
  "_name_or_path": "stabilityai/stable-diffusion-2",
  "feature_extractor": [
    null,
    null
  ],
  "requires_safety_checker": false,
  "safety_checker": [
    null,
    null
  ],
  "scheduler": [
    "diffusers",
    "EulerDiscreteScheduler"
  ],
  "text_encoder": [
    "transformers",
    "CLIPTextModel"
  ],
  "tokenizer": [
    "transformers",
    "CLIPTokenizer"
  ],
  "unet": [
    "diffusers",
    "UNet2DConditionModel"
  ],
  "vae": [
    "diffusers",
    "AutoencoderKL"
  ]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.06it/s]
  0%|          | 0/50 [00:00<?, ?it/s]
Steps:   0%|          | 0/5 [00:00<?, ?it/s]

Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.30it/s]
Steps:   0%|          | 0/5 [00:00<?, ?it/s]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.12it/s]
  0%|          | 0/50 [00:00<?, ?it/s]
Steps:   0%|          | 0/5 [00:00<?, ?it/s]
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]

Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.04it/s]
Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:00<00:00,  4.12it/s]
Steps:   0%|          | 0/5 [16:16<?, ?it/s]
Steps:   0%|          | 0/5 [03:00<?, ?it/s]
Steps:   0%|          | 0/5 [00:00<?, ?it/s]unet/diffusion_pytorch_model.safetensors not found
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.31it/s]
  0%|          | 0/50 [00:00<?, ?it/s]
Steps:   0%|          | 0/5 [00:00<?, ?it/s]
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.00it/s]
